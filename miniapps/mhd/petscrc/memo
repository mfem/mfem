run:
exMHD -r 2 -o 2 -tf 3 -vs 100 -dt .002
exMHD -r 2 -tf 10 -vs 50 -dt .004 -i 2

Parallel AMR run:
mpirun -n 16 exAMRMHDp -m xperiodicR1.mesh -rs 0 -rp 2 -o 4 -tf 8 -vs 200 -dt .0001 -i 3 -amrl 3 -ltol 2e-3 -derefine

parallel run:
mpirun -n 4 exMHDp -rs 4 -o 2 -tf 3 -vs 100 -dt .001
mpirun -n 4 exMHDp -rs 4 -tf 10 -vs 200 -dt .001 -i 2
mpirun -n 4 exMHDp -m xperiodicR3.mesh -rs 0 -o 4 -tf 8 -vs 100 -dt .001 -i 3

case 3 example run
mpirun -n 8 exMHDp -m xperiodic.mesh -rs 3 -o 4 -tf 8 -vs 100 -dt .001 -i 3
mpirun -n 8 exAMRMHDp -m xperiodicR1.mesh -rs 0 -rp 2 -o 4 -tf 8 -vs 200 -dt .0001 -i 3 -amrl 3 -ltol 2e-3 -derefine


serial AMR quick test:
./exAMRMHD -m xperiodicR3.mesh -r 0 -o 4 -tf .025 -vs 10 -dt .0001 -i 3 -amrl 3 -ltol 1e-3

Parallel AMR quick test:
./exAMRMHDp -m xperiodicR3.mesh -rs 0 -o 4 -tf .025 -vs 10 -dt .0001 -i 3 -amrl 3 -ltol 1e-3

plot:
glvis -m refined.mesh -g current.sol -k "mmcRjl" -ww 800 -wh 800
glvis -m refined.mesh -g psiPer.sol -k "mmc" -ww 800 -wh 800
glvis -m refined.mesh -g phi.sol -k "mmc" -ww 800 -wh 800
glvis -m refined.mesh -g omega.sol -k "mmc" -ww 800 -wh 800


glvis -m refined.mesh -g psiPer.sol -k "mmRjl" -ww 1000 -wh 1000

parallel plot:
glvis -np 4 -m mesh -g sol_omega -k "mmc" -ww 800 -wh 800



2/27
Testing supg without fd-fem how will it do on stretched grid??

Let's focus on this run:
../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 2 -tf 5 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6

fd vs supg vs none

Isn't it strange if I have stretched grid to compute eleLength?? Maybe that is the reason? But it is only off by a factor of 2 (it explains why if I stretched aggressively, things are not working).

At t=4; use supg (only on omega) and fd on (psi) gives a good solution

Without fd, the solutions with supg become very bad (omega). The original bad results are probably due to under resolved.

Adding both supg and fd on psi seems not a good idea? Why? Did I have a bug somewhere in computing y2?

2/26
OMG, I truned off the stabilization term when testing stretched grid!!

So the stretched grid works well for 1e-4 without stabilization term (in other words we could capture it through amr)!
the preconditioner with the stabilization term does not like the stretched grid!! Maybe adjust the fd term in jfnk?

res=1e-4 preconditioner does not like stretched grid; 
res=1e-6 it is better

uniform r5o3 stuck at t=5.6 (add more supg!)

xperiodic-stretched.mesh r5o3


2/21
two possibilities where the preconditioner fails:
Sc  (too oscillatory for omega, or underresolved? see case3StretchedR5o3res1e-5/log_study)
ARe (also too oscillatory omega or maybe underresolved??) see case3StretchedR5o5res1e-6 (no plot) or case3R1StretchedR5o3res1e-5 (no plot yet)

case3StretchedR5o3res1e-5
vs
case3R1StretchedR5o3res1e-5

Good results (1e-4):
case3stretchedR5o4

1e-6
xperiodic-newR1-stretched.mesh -rs 5 -rp 0 -o 3 with dt=0.1:
time=1924.11

what to do?
1. do more refinement study
2. do more tests with larger nu

2/20
mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -no-vis -vs 1 -tf 4.9 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6 > log3.out

1e-6
r5o4 stuck at t=5.0 (probably under-resolved)
r5o5 ??

1e-6 (more stretched)
R1r5o3 failed to converged!!

newR1r5o3 also fails to converge

1e-5

what about resistive=1e-5 and viscosity=1e-4

1e-6
r6o4 runs fine to t=2


some ideas to try:
do more stretching (this appears to be a bad idea R1-stretched is bad idea, maybe made the stretching more smooth??)
try add the B diffusion on omega



2/7
mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 5 -i 3 -no-vis -vs 1 -tf 8 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6 > log1.out &

mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 1 -o 3 -i 3 -no-vis -vs 1 -tf 1 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5 > log1.out &

mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 1 -o 4 -i 3 -no-vis -vs 1 -tf 1 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5 > logR6o4.out

mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -no-vis -vs 1 -tf 4.9 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6 >log2.out

2/6
testing amr criterion:
(stuck at t=6.8)
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-8 -derefine -resi 1e-4 -visc 1e-4 -refs 4

mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-8 -derefine -resi 1e-4 -visc 1e-4 -refs 8

mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 1 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-8 -derefine -resi 1e-6 -visc 1e-6 -refs 8

mpirun -n 42 imMHDp -m Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -vs 2 -tf 2 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4 

mpirun -n 42 imMHDp -m Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -vs 2 -tf 2 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6


mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -vs 2 -tf 2 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4

mpirun -n 42 imMHDp -m Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -vs 2 -tf 2 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6

mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -no-vis -vs 1 -tf 8 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4

mpirun -n 48 ../imMHDp -m ../Meshes/xperiodic-stretched.mesh -rs 5 -rp 0 -o 4 -i 3 -no-vis -vs 1 -tf 2 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5

this looks good at t=2
case3StretchedR5o4res1e-5

issue at t=5.4!! (this is underresolved)
case3StretchedR5o3res1e-5

ok:
case3StretchedR5o3
case3stretchedR5o4

2/5
mpirun -n 48 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 5 -rp 1 -o 3 -i 3 -no-vis -tf 5 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 1 -ltol 1e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 10 -yrange

oscillations here??
mpirun -n 12 exMHDp -m Meshes/xperiodic-new.mesh -rs 5 -o 3 -tf 1 -i 3 -vs 100 -dt .001 -resi 1e-6 -visc 1e-6

2/2
ideas to try: 
1. no supg, resi=1e-4? t=8? but with amr
this can run but supg definitely helps a lot!

2. turn of supg after a while?
3. reduce supg for resi=1e-4

4. try to check if reduce nonconforming level would help??
set nc_limit=1 seems help a lot!!
resi=1-4 it can run up to t=6

mpirun -n 32 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 10

np=32 got stuck at t=7.2 (probably the second peak time!)

it reaches the peack around t=6.7-6.8

this works well:
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 10

this got stuck at t=4.8, wth? (something happens at this time??)
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-5 -visc 1e-5 -refs 10

np=32 it got stuck at t=4.0
np=42 r=5, it got stuck at t=2.5, maybe increase wp to 5e-3
np=42,r=5, after modification, it got stuck at 4.4

try this (stuck at t=3):
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 2 -tf 4 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 5e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 4 -yrange

mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 2 -tf 4 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 5e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 10 -yrange

(this got stuck at t=4 after mesh is refined; the adding a fine mesh is a bad idea; can we do some prepacking??)
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 2 -tf 6 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 5e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 10 -yrange

try (got stuck again at t=4; can we do prepacking??):
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 2 -tf 6 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 10 -yrange

this seems got stuck at t=3.2
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 6 -rp 0 -o 3 -i 3 -vs 2 -tf 6 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 5e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 10 -yrange

try this:
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 6 -rp 0 -o 3 -i 3 -vs 2 -tf 6 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 2 -ltol 1e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 10 -yrange

try this:
mpirun -n 42 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 7 -rp 0 -o 3 -i 3 -vs 2 -tf 6 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 1 -ltol 1e-4 -derefine -resi 1e-6 -visc 1e-6 -refs 10 -yrange

try this (this is fine but very under-resolved):
mpirun -n 42 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 2 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -resi 1e-4 -visc 1e-4 

1/29
mpirun -n 46 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 5 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 4
this got stuck at t=2.0?? with rc_debug

mpirun -n 46 imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 5 -tf 8 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 4

preconditioner becomes worse after derefine or mesh changed in general??

okay (the plotting routine will affect AMR and may affect convergence, a bug???):
mpirun -n 44 ../imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -vs 1 -tf 4 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 10

mpirun -n 48 ../imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 3 -i 3 -no-vis -tf 8 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 10 > log.out

1/22
try to investigate why the hyprediffusion is so stiff in an explicit scheme (1e-6 for resistivity)
mpirun -n 12 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 100 -tf 4 -dt .0005 -resi 1e-6 -visc 1e-6


explicit scheme without FD is stable for dt=.008 with rs=3
explicit scheme without FD is stable for dt=.004 with rs=4
explicit scheme without FD is stable for dt=.002 with rs=5
explicit scheme without FD is stable for dt=.001 with rs=6


with the diffusion term, it is not working with dt=.001 with rs=4 and alpha=.2

it becomes unstable somewhere between t = .72 and .73
srun -n 36 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -tf .75 -dt .0005 -resi 1e-6 -visc 1e-6

dt=.0002 seems okay
dt=.0004 seems okay (10 times smaller)


1/10
study FDFEM

tau = h^2/invtau;
this needs a much smaller time step for explicit scheme
dt=1e-6

use tau = .0001/invtau and a smaller time step for the hyprediffusion term (works much better):
mpirun -n 12 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 100 -tf 4 -dt .0005 -resi 1e-6 -visc 1e-6

can it run longer? Yes for the explicit scheme
will other terms matter? No.

Now consider the implicit case:
mpirun -n 12 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 2 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell

if ALPHA=1e-2; the preconditioner not good for resi=1e-6
if ALPHA=1e-3; the preconditioner seems ok

ALPHA=1e-3 corresponding to hyprediffusion parameter of 1.5e-8
In explicit schemes, diffusion paramter was 2.5e-8

ALPHA=5e-3 with dt=.05; preconditioner seems work
ALPHA=1e-2 with dt=.05; preconditioner seems work hyperdiffusion of 1.2e-7
ALPHA=5e-2 with dt=.05; preconditioner seems work hyperdiffusion of 6e-7 (80-100 iterations)

with o=2 things become strange with the stabilzed term turned on
mpirun -n 12 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 2 -i 3 -vs 2 -tf 1 -dt .05 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -resi 1e-6 -visc 1e-6

it is probably a bad idea to use hyperdiffusion p2


it is okay to use resi=1e-3 (30-40 iterations in total), but not resi=1e-6




12/23
this is not quite good even with supg (both element max or a local value)
imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 100 -tf 1 -dt .002 -resi 1e-6 -visc 1e-6

12/22
note that different resi gives different E0rhs!
at least stabilization terms has impact now but the stabilization parameter is funny

12/12
does the explicit supg make sense? I am not sure now...
maybe remove all other operators and only use (V.grad u, V.grad h) to make sure diffusion is doing something
dt/2(V.grad u, V.grad h) should be a reasonable diffusion term

12/10
testing:
mpirun -n 4 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 100 -tf .002 -dt .002 -resi 1e-6 -visc 1e-6

12/6
some notes for the operators:
grad(u)=J^{-T} hat_grad(hat_u)
J^{-1}=(1/det(J)) adj(J)

grad(u)*v*Weight()
but Weight()=det(J) and therefore only adjJ is needed

12/1
MatZeroRowsColumnsIS is eliminating zeros in my jacobi? I assume I could skip it if I use EliminateEssentialBC through mfem?
Should not I used a derived class from PetscBCHandler? So all those values should be protected for easier access?

11/20
there is a bug in the boundary condition in the implicit case (I probaby need to use bchandler).
[I 0] |x| = |a| 
[D A] |y|   |b|
is not necessary equivalent to
[0 0] |x| = |0| 
[D A] |y|   |b|
by setting the initial guess being x=a

11/19
resi=1e-4 imAMR the first run stuck at t=6.8 (the mesh looks good; but it probably needs slightly more damping)

mpirun -n 32 ../imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 5 -tf 8 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-4 -visc 1e-4 -refs 10

resi=1e-5 very oscillatory; the solve got stuck at 4.2
mpirun -n 32 ../imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 5 -tf 8 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine -resi 1e-5 -visc 1e-5 -refs 10

mpirun -n 32 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 5 -tf 1 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -resi 1e-5 -visc 1e-5

low-order
mpirun -n 42 ../imAMRMHDp -m ../Meshes/xperiodic-new.mesh -rs 5 -rp 0 -o 2 -i 3 -vs 5 -tf 8 -dt .1 -usepetsc --petscopts rc_debug -s 3 -shell -amrl 3 -ltol 1e-4 -derefine -resi 1e-6 -visc 1e-3 -refs 10

11/18
debug:
-m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -no-vis -tf .5 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3

a bug fixed in ParBilinearForm define

this is ok (using refine step=4)
mpirun -n 16 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 1 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3

this is ok (I may try a fine mesh to start with instead; here the derefine is aggressive):
mpirun -n 16 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 1 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3 -derefine


11/16
avoid SetFromTrueVector() when mesh is changed; this gives the wrong answer!


11/15
testing:
mpirun -n 4 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 1 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3


11/13
add a fixed adaptive mesh to imAMRMHDp;

this is okay
mpirun -n 4 imMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 2 -i 3 -vs 1 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell

this is okay for a fixed adaptive mesh (no significant impact)
mpirun -n 4 imAMRMHDp -m Meshes/xperiodic-new.mesh -rs 4 -rp 0 -o 3 -i 3 -vs 1 -tf 8 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell -amrl 3 -ltol 1e-3


11/4
move back to imAMRMHDp

things need to be changed: 
*1. j0 is owned in main (so that j0 can be updated in AMRUpdate)
*2. add j in main for amr estimator
3. initialize reducedSystemOperator after mesh is changed


9/16
for sl=10^3 and o=3, I cannot solve preconditioner too tight:
    for r=6, 7, 8
    mffd 1e-3
    rtol=1e-3 in small matrices is better than 1e-5 
    mass matrix should always be 1e-6

    for r=7, resi=1e-4
    it is more important to use 1e-2 in mffd
    for r=8, resi=1e-4
    I cannot find a working parameter set so far
    what if I recuded mffd to 5e-2

I am wondering if it could help o=2 s=10 case
1e-3 is okay for r=7, r=8, not okay for r=9

8/27
revisit exAMR

this seems working
exAMRMHD -m Meshes/xperiodicR3.mesh -r 0 -o 3 -tf 8 -vs 10 -dt .0002 -i 3 -amrl 3 -ltol 1e-3

o=3 derefine seems have very small impact;
exAMRMHD -m Meshes/xperiodicR3.mesh -r 0 -o 3 -tf 8 -vs 10 -dt .0002 -i 3 -amrl 3 -ltol 1e-3 -derefine

this is okay np=4,8 (it is possible because the previous AMR setup has some issue; it could be derefine was too aggressive):
mpirun -np 4 exAMRMHDp -m Meshes/xperiodicR3.mesh -rs 0 -o 3 -tf 1 -vs 10 -dt .0002 -i 3 -amrl 3 -ltol 1e-3 -derefine

this becomes strange (this is not the same because derefine is different!!)
mpirun -np 4 exAMRMHDp -m Meshes/xperiodic.mesh -rs 3 -o 3 -tf 1 -vs 10 -dt .0002 -i 3 -amrl 3 -ltol 1e-3




o2 is not working

8/26
exAMRMHD fails for o=3? Diverged pcg? time steps are probably too large

8/24
test rs=9: will reduce stiffness rtol=1e-5 be better?

8/23
resi=1e-3 is okay for rs 7, 8 not 9
resi=1e-4 is okay for rs 7, 8, 9

for resi=1e-4
for rs<=7 rtol=1e-5 is good
for rs>7 rtol for Schur needs to be 1e-6

rs=9
change stiffness to 1e-5 ok
mass matrix has to be inverted accurately

iter=2,3 not good


8/20
mpirun -n 16 imMHDp -m Meshes/xperiodic-new.mesh -rs 6 -rp 0 -o 2 -i 3 -vs 1 -tf 1 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell

potential improvements (minor):
1. do schur complement inside petsc
2. not assemble matrix when impose boundary condition

8/19
this seems work:
srun -n 16 imMHDp -m Meshes/xperiodic-square-new.mesh -rs 6 -rp 0 -o 2 -i 2 -vs 1 -tf 5 -dt 5 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell
srun -n 16 imMHDp -m Meshes/xperiodic-square-new.mesh -rs 7 -rp 0 -o 2 -i 2 -vs 1 -tf 5 -dt 5 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell
srun -n 32 imMHDp -m Meshes/xperiodic-square-new.mesh -rs 8 -rp 0 -o 2 -i 2 -vs 1 -tf 5 -dt 5 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell


8/17
new tests:
mpirun -n 4 exMHDp -m Meshes/xperiodic-new.mesh -rs 4 -o 2 -tf 1 -vs 100 -dt .001 -i 3
mpirun -n 4 exMHDp -m Meshes/xperiodic-square-new.mesh -rs 2 -o 2 -tf 3 -vs 100 -dt .001

redo:
64*64
mpirun -n 4 imMHDp  -m Meshes/xperiodic-square-new.mesh -rs 4 -o 2 -i 2 -tf 10 -dt 5 -s 3 -usepetsc --petscopts petscrc/rc_full -shell

test:
srun -n 16 imMHDp -m Meshes/xperiodic-square-new.mesh -rs 6 -rp 0 -o 2 -i 2 -vs 1 -tf 10 -dt 5 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell
mffd_err=1e-3
r=6 (256x256): iter=60 to 80
mffd_err=1e-3
r=7 (512x512): iter=134, 113, 105, 143; some norms are off
mffd_err=1e-2
r=7 (512x512): iter=80, 103, 107, 143; norms are close
r=8 : iter=187, , , ; norms are close
mffd_err=1e-1
r=8 : iter=130, 155, 146, ; norms are close

-mat_mffd_err 1e-1 -mat_mffd_type ds -mat_mffd_umin 1e-5 
r=7 (512x512): iter=112, not working...

from info?? 
[0] MatMult_MFFD(): Current differencing parameter: 6.962523027760e-09
[0] MatMult_MFFD(): Current differencing parameter: 2.757792494633e+00

will o3 scale better?



8/13
srun -n 16 imMHDp -rs 6 -rp 1 -o 2 -i 2 -vs 1 -tf 5 -dt 5 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell
key parameters:
-snes_linesearch_type bt
-mat_mffd_type wp
-mat_mffd_err 1e-3
-snes_ksp_ew_rtol0 0.3
-snes_ksp_ew_rtolmax 0.9

r=8 can be solved by:
-mat_mffd_err 1e-2

8/11
try this:
mpirun -n 4 imMHDp -rs 5 -o 3 -i 2 -tf 10 -dt 2 -vs 1 -no-vis -usepetsc --petscopts petscrc/rc_debug -s 1 -shell
snes_rtol=1e-4 amg tol=1e-5
time 2281

add SDIRK and mid-point stepping

this is okay (change mesh to a single mesh)

ok
mpirun -n 6 imMHDp -m meshes/xperiodic1.mesh -rs 4 -rp 2 -o 3 -i 3 -vs 1 -tf .1 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell
  total number of linear solver iterations=15

not ok
mpirun -n 6 imMHDp -m meshes/xperiodic1.mesh -rs 5 -rp 2 -o 3 -i 3 -vs 1 -tf .1 -dt .1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell

ok
mpirun -n 4 imMHDp -rs 4 -rp 2 -o 2 -i 2 -vs 1 -tf 5 -dt 5 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell

ok
mpirun -n 6 imMHDp -rs 5 -rp 2 -o 2 -i 2 -vs 1 -tf 1 -dt 1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell
let mffd be 1e-4 becomes much better?!

check the allocation part, is it slow??
mpirun -n 6 imMHDp -rs 5 -rp 3 -o 2 -i 2 -vs 1 -tf 1 -dt 1 -usepetsc --petscopts petscrc/rc_debug -s 3 -shell

mat_mffd_err 1e-4 works fine for r=7
mat_mffd_err 1e-3 works fine for r=8! it works for dt=5 of r=7!


8/8
probably need to change Sc to nontranspose version? No
Changing mat_mffd_err 1e-5 helps significantly!

this works:
 mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -vs 1 -tf 250 -dt 5 -usepetsc --petscopts petscrc/rc_debug -s 1 -shell
 mpirun -n 4 imMHDp -m xperiodic.mesh -rs 4 -o 3 -i 3 -vs 1 -tf 8 -dt .2 -usepetsc --petscopts petscrc/rc_debug -s 1 -shell

use richardson as ksp if one wants to use hypre as the solver in petsc

8/7
inexac Newton?

preconditioner needs improvements in solving omega (increase regularity)

this does not perform well
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -vs 1 -tf 5 -dt 5 -usepetsc --petscopts rc_debug -s 1 -shell

inexact newton is ok for case 1
mpirun -n 4 imMHDp -rs 4 -o 2 -vs 1 -tf 3 -dt .1 -usepetsc --petscopts rc_debug -s 1 -shell
ok for case 3
mpirun -n 4 imMHDp -m xperiodic.mesh -rs 4 -rp 0 -o 3 -tf 1 -vs 1 -dt .1 -i 3 --usepetsc --petscopts rc_debug -s 1 -shell

icase 2:
resitivity=10^4; dt = 2 ok
resitivity=10^3; dt = 1 ok
resitivity=10^3; dt = 2 (fgmres is slightly better; will take more ksp solves)

maybe I should use MatCreateSchurComplement?

8/6
compare
run -n 4 imMHDp -rs 2 -o 2 -tf 3 -vs 100 -dt .002 -usepetsc --petscopts rc_jfnk -s 1 -shell
vs
run -n 4 imMHDp -rs 2 -o 2 -tf 3 -vs 100 -dt .002 -usepetsc --petscopts rc_full -s 1 -shell

this seems working:
srun -n 4 imMHDp -rs 4 -o 2 -tf 3 -vs 100 -dt .1 -usepetsc --petscopts rc_full -s 1 -shell

when iter=10, whis works
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -tf 1 -dt .02 -usepetsc --petscopts rc_full -s 1 -shell

when iter=5, this is ok
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -tf 1 -dt .01 -usepetsc --petscopts rc_full -s 1 -shell

when iter=8, this is ok
mpirun -n 4 imMHDp -m xperiodic.mesh -rs 4 -rp 0 -o 3 -tf 1 -vs 50 -dt .01 -i 3 --usepetsc --petscopts rc_full -s 1 -shell

My time step is smaller than pixie2d??

Possible fix:
1 use fieldsplit
2 use inexact Newton

I am doing GMRES for nested mat or jfnk?? jfnk

iter=5 is ok if GMRES is turned on but I am not sure I am doing the correct GMRES

this works (with iter=5):
-snes_monitor
-snes_mf_operator
-snes_max_it 20
-snes_rtol 5e-6
-snes_converged_reason 
-snes_linesearch_type l2
#
# use inexact newton based on pixie2d:  
# ierr = SNESKSPSetParametersEW(snes,3,tolgm,0.9,0.9,1.5,1.5,0.1)
-snes_ksp_ew
-snes_ksp_ew_version 3
-snes_ksp_ew_rtol0 5e-6
-snes_ksp_ew_rtolmax 0.9
-snes_ksp_ew_gamma 0.9
-snes_ksp_ew_alpha2 1.5
-snes_ksp_ew_alpha2 1.5
-snes_ksp_ew_threshold 0.1
#
-ksp_type gmres
-ksp_converged_reason
-ksp_rtol 5e-6
-ksp_monitor_true_residual
#======Stiffness matrix======
-s1_ksp_rtol 1e-6
-s1_ksp_type cg
-s1_pc_type hypre
#======Schur matrix======
-s2_ksp_rtol 1e-6
-s2_pc_type hypre
#======Mass matrix======
-s3_ksp_rtol 1e-6
-s3_ksp_type cg

ok:
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -vs 1 -tf 20 -dt 1 -usepetsc --petscopts rc_full -s 1 -shell
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -vs 1 -tf 20 -dt 5 -usepetsc --petscopts rc_full -s 1 -shell

what kind of newton solver used in pixie2d?


8/5
The current way of allocating hypre matrices may be slow in assembly

Alternatively, I could try:
VecReciprocal(diag);
MatDiagonalSet
MatPtAP

Try use Add to define some matrices (in SetParameters)

7/29
fix useFull=false (I need to use ParAdd and I cannot delete BilinearForm)
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -tf 1 -dt .001 -usepetsc --petscopts rc_jfnk -s 1 -shell

ok:
srun -n 4 imMHDp -rs 4 -o 2 -i 2 -tf 1 -dt .001 -usepetsc --petscopts rc_jfnk -s 1 -shell
srun -n 4 imMHDp -rs 2 -o 2 -tf 3 -vs 100 -dt .002 -usepetsc --petscopts rc_jfnk -s 1 -shell

7/13
in GetGradient:
assemble Nv and Nb (let diag = 0)
construct Sc = ASl + Nb^T D^-1 Nb: follow ex5p
   M->GetDiag(*Md);
   MinvBt->InvScaleRows(*Md); 
and pass
ARe Nb 0
0   Sc 0
K   0  M

there is something wrong in the Jacobi iteration:
missed one right hand side!
missed the correction for the second equation in my Jacobi iteration!

Save Mdt+KSl and Mdt+KRe; adjust only when timestep is changed

working example:
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -tf 1 -dt .001 -usepetsc --petscopts rc_jfnk -s 1 -shell
vs
mpirun -n 4 imMHDp -rs 4 -o 2 -i 2 -tf 1 -dt .001 -s 2 

6/24
working on full preconditioner: follow ex5p.cpp

option 1:
GetDiag for M/dt+SRe and GetGiag for Nv
option 2:
use MatGetDiagonal (eaiser)
DRe and DSl are assembled in parallel

Step 1: Try this instead (this will give me HypreParMat for diffusion operartor):
   S0->Assemble();
   S0->EliminateEssentialBC(ess_bdr);
   S0->Finalize();
   HypreParMatrix * matS0   = S0->ParallelAssemble();    delete S0;

Mult (HypreParVector &x, HypreParVector &y, double alpha=1.0, double beta=0.0)
   y = alpha * A * x + beta * y
So addMult should be
matS0.Mult( x, y, 1.0, 1.0)

then delete DSl DRe

tried but this is not going to work because ParallelAssemble removes all the terms w.r.t essential dofs. However, this should not affect the preconditioner

Step 2: double check notes; ok
Step 3: build two operators and add capibility of changing time step (variable time stepping is left for later)
Step 4: implement Jacobi iteration in pcshell


Nb->ParallelAssemble;
Md = new HypreParVector(MPI_COMM_WORLD, Nb->GetGlobalNumRows(),
                              Nb->GetRowStarts());
HypreParVector *Md = NULL;
M->GetDiag(*Md);
MinvBt->InvScaleRows(*Md);
S = ParMult(B, MinvBt);

the only question is Jacobi iteration. Maybe, I could rescale in pcshell
Yes, use VecPointwiseDivide to implemented Diag_Re

There are two ways to build Schur complement: in mfem or petsc. I will build it in mfem for now

How many Jacobi iterations? fixed of 2-4 (no need to set stoppage)
What would be a good stoppage critera for Jacobi iteration?

Be careful for the negative signs in the preconditioner!
Should I do something special for the boundary? 
=======
6/25
try to reproduce results:
srun -n 4 imMHDp -rs 4 -tf 1 -vs 200 -dt .001 -i 2 -no-vis

but 
EliminateEssentialBC(ess_bdr) will not generate the matrix I need

6/16
the explicit and implicit solvers are comparable with pcshell:
(this uses a very bad linear solver, so it converges very slow)

ok:
###
srun -n 1 ../imMHDp -rs 2 -o 2 -tf .05 -vs 100 -dt .001 -m ../xperiodic-square.mesh
###vs###
srun -n 1 imMHDp -rs 2 -o 2 -tf .05 -vs 40 -dt .00025 -usepetsc --petscopts rc_mfop -s 1 -no-vis
-snes_monitor
-snes_mf_operator
-snes_max_it 10
-snes_rtol 1e-5
-ksp_type minres
-ksp_rtol 1e-5
-ksp_atol 1e-20
-pc_type jacobi

ok:
###
srun -n 4 ../imMHDp -rs 4 -o 2 -tf .0001 -vs 100 -dt .0001 -m ../xperiodic-square.mesh
###vs###
srun -n 4 imMHDp -rs 4 -o 2 -tf .0001 -vs 40 -dt .00002 -usepetsc --petscopts rc_mfop -s 1 -no-vis


ok:
mpirun -n 4 ../imMHDp -rs 4 -tf .001 -vs 200 -dt .001 -i 2
srun -n 4 imMHDp -rs 4 -o 2 -tf .001 -dt .00002 -i 2 -usepetsc --petscopts rc_mfop -s 1 -no-vis
petsc flag:
-snes_monitor -snes_mf_operator
-snes_max_it 10
-snes_rtol 1e-5
-ksp_type minres
-ksp_rtol 1e-5
-ksp_atol 1e-20
-pc_type jacobi

not ok:
srun -n 1 imMHDp -rs 2 -o 2 -tf .05 -vs 40 -dt .00025 -usepetsc --petscopts rc_jfnk -s 1 -shell

sub is deleted in petsc!!

add a mini example:

this works fine:
srun -n 1 mini -rs 2 -tf .00004 -dt .00002 --petscopts rc_mfop
srun -n 4 mini -rs 4 -tf .00004 -dt .00002 --petscopts rc_mfop

this is not working:
srun -n 1 mini -rs 2 -tf .00004 -dt .00002 --petscopts rc_jfnk -shell
srun -n 4 mini -rs 4 -tf .00004 -dt .00002 --petscopts rc_jfnk -shell

counter part:
srun -n 1 imMHDp -rs 2 -o 2 -tf .00004 -dt .00002 -usepetsc --petscopts rc_jfnk -s 1 -shell
srun -n 4 imMHDp -rs 4 -o 2 -tf .00004 -dt .00002 -usepetsc --petscopts rc_jfnk -s 1 -shell


6/11
debug:
srun -n 1 imMHDp -rs 2 -o 2 -tf 3 -vs 100 -dt .002 -usepetsc --petscopts rc_jfnk -s 1 -no-vis




6/6
Is the iterative mode working for PetscNonlinearSolver??
I think so. However, preconditioner should not use iterative mode, since it solves for du!

6/5
PetscParMatrix *pA;
oh.Get(pA);
hypre_ParCSRMatrix *parcsr;
MatHYPREGetParCSR(*pA,&parcsr);

Could I just use MATAIJ in jacType? I do not see the point to use MATHYPRE
Do I construct a MyBlockSolver every time step or not? No

5/29
o ReducedSystemOperator 
o link it with petsc
o getgradient
o preconditionerfactory 
o add kspsolve into pcshell

class MyBlockSolver : public mfem::Solver
{
   MyBlockSolver(OperatorHandle oh) { //constructor
  PetscParMatrix *PP;
  // Get the PetscParMatrix out of oh. I showed you how to do it
  Mat P = *PP; // type cast to Petsc Mat
  MatNestGetSubMats(P,&N,&M,&sub)// sub is an N by M array of matrices
  // Create your own internal KSP objects to handle the subproblems
  // Create PetscParVectors as placeholders internal_x and internal_y
}
  Mult(const Vector & x, Vector &y)
  {
   internal_x.PlaceArray(x.GetData()); // no copy, only the data pointer is passed to PETSc
 
   internal_y.PlaceArray(y.GetData());


  MatNestGetIs(...) // get the index sets of the blocks
  for i = 1:3
    VecGetSubVector(internal_x,index_set[i],&blockx)
    VecGetSubVector(internal_y,index_set[i],&blocky)
   KSPSolve(kspblock[i],blockx,blocky)

    VecRestoreSubVector(internal_x,index_set[i],&blockx)
    VecRestoreSubVector(internal_y,index_set[i],&blocky)
  }


 
   internal_x.ResetArray();
 
   internal_y.ResetArray();



  }
}

Solver * PreconditionerFactory::NewPreconditioner(OperatorHandle oh)
{
  return new MyBlockSolver(oh);
}

Q:
Could I call mfem linear solver in reducedsystem::mult?
Is it better to pass pointers of HypreParMatrix to reducedsystem? I do not think reducedsystem owns those objects.
Similarly, I could pass M_solver into reducedsystem.

5/22
Try this (is that for parallel?)
   BlockOperator *darcyOp = new BlockOperator(block_trueOffsets);
   darcyOp->SetBlock(0,0, M);
   darcyOp->SetBlock(0,1, BT);
   darcyOp->SetBlock(1,0, B);

This is for serial:
   BlockMatrix A = new BlockMatrix( offsets );
   A->SetBlock(0,0, &A00);
   A->SetBlock(0,1,  &A01);
   A->SetBlock(1,0,  &A10);

In pcshell, I need to
    KSPSetOperators(ksp1,Mdt,Mdt);
    KSPSolve(ksp1, b2, x2)
    KSPSolve(ksp1, b3, x3)
    
    MatMult(Mdt,x3,bTmp);
    bTmp=bTmp*dt+b1;
    KSPSetOperators(ksp2,K,K);
    KSPSolve(ksp2, b2, x2)

See
file:///Users/qtang/software/petsc-3.10.2/src/ksp/ksp/examples/tutorials/ex9.c.html

Where should I initialize KSP?
    KSPCreate(PETSC_COMM_WORLD,&ksp1);
    KSPSetFromOptions(ksp1);
    KSPAppendOptionsPrefix(ksp2,"s1_");
    KSPCreate(PETSC_COMM_WORLD,&ksp2);
    KSPAppendOptionsPrefix(ksp2,"s2_");
    
    Define three ksp in __mfem_pc_shell_ctx
    Then destroy them in __mfem_pc_shell_destroy

what would be a good initial guess? Use zero for now.



5/21
switch solver to true dofs for ImplicitSolve

5/20
swich J back to auxilary variable (keep the original implement in exMHD for debugging purpose)


5/17
Issues:
x it has to assemble nv and nb each time step, not sure how to do it implicitly (am I doing matrix-free for those two? Probably not)

implemented a new approach based on LinearForm, but it is slower

todo for imMHDp
1. treat J as a true auxiliary variable
2. implement a new class for implicit scheme and reduced operator


5/15
Q1. what happens to GetGradient when factory is used? It is called through __mfem_snes_jacobian
2. Where is Mult implemented now? It is implemented through J->Mult from GetGradient
3. reducedOperator->Mult is something else (not the same)

see submatrix in (a good example on multiphysics coupling)
https://www.mcs.anl.gov/petsc/petsc-current/src/snes/examples/tutorials/ex28.c.html

Question: how could I access the block vec from y??
MatGetLocalSubMatrix
VecGetLocalVector
Those will determine the local matrix and local vectors

Define the linearform operator:
the linear integral order is probably 3k/2
define a special coefficient by myself
then define a linearform and assemble


5/11
-snes_mf_operator is working 


5/6
mpirun -n 1 exAMRMHDp -m xperiodic.mesh -rs 3 -o 4 -tf 8 -vs 10 -dt .0001 -i 3 -amrl 3 -ltol 1e-3
mpirun -n 4 exAMRMHDp -m xperiodic.mesh -rs 3 -o 4 -tf 8 -vs 10 -dt .0001 -i 3 -amrl 3 -ltol 1e-3

derefine in parallel amr working:
mpirun -n 16 exAMRMHDp -m xperiodicR1.mesh -rs 0 -rp 2 -o 4 -tf 8 -vs 200 -dt .0001 -i 3 -amrl 3 -ltol 2e-3 -derefine

5/4
on nersc debug mode has issue (fixed)
make config MFEM_USE_MPI=YES MFEM_DEBUG=YES MPICXX=CC
standard is ok:
make config MFEM_USE_MPI=YES MFEM_DEBUG=NO MPICXX=CC

serial config:
make config MFEM_USE_MPI=NO MFEM_DEBUG=NO CXX=CC

5/3
1. Try derefine
2. Prallel AMR

Consider:
JFNK
Poisson Bracket
SMG
Mixed FEM


4/30
Test it on nersc
srun -n 32 ../exMHDp -m ../xperiodic.mesh -rs 6 -rp 0 -o 3 -tf 1 -vs 2500 -dt .0002 -i 3 -no-vis -visit

profiling this case:
mpirun -n 48 ../exMHDp -m ../xperiodic.mesh -rs 6 -rp 1 -o 3 -tf .1 -vs 1000 -dt .00005 -i 3

4/29
After fixing a bug, parallel code works well:
mpirun -n 96 ../exMHDp -m ../xperiodic.mesh -rs 6 -rp 0 -o 3 -tf 8 -vs 2500 -dt .0002 -i 3 -no-vis -visit

4/27
parallel code appears working

for icase=3
need to reduce rel_tol to invert the mass matrix
it is better to use different rel_tol for mass and stiffness matrices

BoomerAMG is also working
mpirun -n 8 exMHDp -m xperiodic.mesh -rs 3 -rp 2 -o 3 -tf 1 -vs 50 -dt .001 -i 3

4/26
the paralle code sometimes work but sometimes give me unstable results

ok:
mpirun -n 2 exMHDp -rs 2 -tf 10 -vs 50 -dt .004 -i 2
mpirun -n 4 exMHDp -rs 3 -o 3 -tf 3 -vs 100 -dt .001 -no-vis

there is probably a bug in the saving function (got some numbers of 1e-312)

I cannot make vis working. Maybe there is a bug in the code?

this works:
mpirun -n 4 exMHDp -rs 2 -rp 2 -tf 10 -vs 50 -dt .0005 -i 2 -visit -no-vis

this fails on mac (there is a bug somewhere) but works on hpcc:
mpirun -n 4 exMHDp -rs 4 -rp 0 -tf .001 -vs 100 -dt .001 -i 2 -no-vis

this also fails:
mpirun -n 4 exMHDp -rs 2 -o 2 -tf .002 -vs 100 -dt .002

this sometimes works and sometimes not (works on intel14 but not intel16):
mpirun -n 4 exMHDp -rs 2 -o 2 -tf 1.5 -vs 100 -dt .002

4/25
there is a strange bug associate with M->Mult (not consistent in serial and parallel)

4/23
try to benchmark against Pixie2d; the ic example give different resutls with the same initial condition
o found the missing source term in the ic case


4/12
parallel version:
mesh -> ParMesh
FiniteElementSpace -> ParFiniteElementSpace
GridFunction -> ParGridFunction
save solution needs to be changed and visual as well

myCoefficient.hpp? probably could stay sound
Change some preconditioner in ResistiveMHDOperator.hpp


4/10
try:
Start with an AMR mesh
Refine every 1000 time steps
./exAMRMHD -m xperiodicR3.mesh -r 0 -o 4 -tf 8 -vs 10 -dt .0001 -i 3 -amrl 3 -ltol 1e-3

in the inner loop of refinement, I could potential do an iteration!

4/9
grid space 5e-3 at x point for island
try back solve Psi through J! not working!
Evolve current? maybe not there yet

do some iterations first and obtain a better initial grid!!

>>>Do AMR grid generation first
./generateAMR -m xperiodicR3.mesh -r 0 -o 4 -i 3 -amrl 3 -ltol 1e-10

4/8
>>>derefiner is a major source to introduce osscillations
Demo: 129->130 (see the oscillations in omega!):
Number of unknowns: 52336
Number of elements in mesh: 3024
step 130, t = 0.013
Derefined mesh...
True V size = 46496
Problem size = 51952
Number of unknowns: 51952
Number of elements in mesh: 3000

>>>K operator plus an interpolation (and grad J) is the secondary issue

try
./exAMRMHD -m xperiodicR3.mesh -r 0 -o 4 -tf 8 -vs 1000 -dt .0001 -i 3 -amrl 3 -ltol 5e-3 -visit

4/6
this is sort of working
 ./exAMRMHD -m xperiodic.mesh -r 3 -o 4 -tf 10 -vs 50 -dt .0001 -i 3 -ltol 1e-3

after adding refinement level, this works better
./exAMRMHD -m xperiodic.mesh -r 3 -o 4 -tf .02 -vs 50 -dt .0001 -i 3 -amrl 3 -ltol 1e-3


4/5
AMR starts to work after I switch vector size total dofs

4/3
there are some issues for AMR:
o KB add more degrees freedom for nonconforming AMR (see by trun off refiner.SetTotalErrorFraction(0.0))
  I can probably fix it with Mark's suggestion.
x When it did uniform refinement, KB is okay but Current is very osscillatory (P4 is better but I am surprised by the results)
  see (by turn on refiner.SetTotalErrorFraction(0.0)):
  ./exAMRMHD -m xperiodic.mesh -r 3 -o 3 -tf 10 -vs 50 -dt .002 -i 3 -ltol 1e-6
x I do not know how to add the AMR levels (probably control through nc_limit)
x Derefine is not tested yet

there are one issue for nonlinear:
x I have to assemble matrix whenever the nonlinear operator called

o it is not doing uniform refinement any more

We may need to have two requests:
o fix KB for nonconforming AMR
o install the nonlinear operator

4/2
this looks okay, although it does some uniform refinement:
./exAMRMHD -r 3 -o 3 -tf 10 -vs 50 -dt .002 -i 2 -ltol 1e-6

island test:
./exAMRMHD -m xperiodic.mesh -r 3 -o 3 -tf 10 -vs 50 -dt .002 -i 3 -ltol 1e-6

amr test is strange:
current is oscillatory
amr can only do uniform refinement

4/1
there is a strange bug: I have to store coefficients for diffusion operator (deleting them to quick leads to issue in assembling later)
not able to get block vector working with update

3/30
working on amr: 
o define a new estimator: error=erro1+ratio*error2
  Note that since the errors are computed to the total error, it is fine as long as ratio>0.
  By default, ratio = 1.

Ideally, I should define a new ThresholdRefiner that refine the union of two refining region
of two solution. This is not hard. But I am not sure how to do similarly for Derefiner.


3/25
fix a bug from boundary condition when compute J=nable Psi (it did not consider boundary condition)

3/23
clean up the explicit code; move on to test AMR (explicit)

Should I remove nodes in the mesh (nodes only for ALE type methods??)?

AMR with explicit time stepping is not very clear
How to do subcyling?


good example on amr:
 ./ex6 -m ../data/fichera.mesh -o 2

3/22 
After fix the rhs boundary bug, OK for wave case
tearing mode looks ok so far

exMHD -r 4 -tf 250 -vs 200 -dt .001 -i 2

3/21
o add visit

for o = 2
the solutions appear to be first order (from omega and phi); it looks okay
there is a boundary layer for omega

for o3 r4, the errors from boundary become very large; this is strange

Test:
exMHD -m ../xperiodic-square.mesh -r 2 -o 2 -tf 3 -vs 50 -dt .001 -visit
exMHD -m ../xperiodic-square.mesh -r 3 -o 2 -tf 3 -vs 100 -dt .0005 -visit

find a potential bug in the boundary: how could I impose dirichlet boundary in the right hand side?

redo those examples:
../exMHD -m ../xperiodic-square.mesh -r 2 -o 3 -tf 3 -vs 100 -dt .001 -visit
../exMHD -m ../xperiodic-square.mesh -r 3 -o 3 -tf 3 -vs 200 -dt .0005 -visit
../exMHD -m ../xperiodic-square.mesh -r 4 -o 3 -tf 3 -vs 400 -dt .00025 -visit


3/20
o fix a ploting issue for order>2
o fix the boundary orientation, I believe the normal has to be outward
o fix the time stepping
o add visual



3/19
the boundary seems working by using but I do not know why it works...

   const int dim=2;
   VectorArrayCoefficient force(dim);
   for (int i = 0; i < dim-1; i++)
   {
      force.Set(i, new ConstantCoefficient(0.0));
   }
   {
      Vector pull_force(fespace.GetMesh()->bdr_attributes.Max());
      pull_force = -1.0;
      force.Set(dim-1, new PWConstCoefficient(pull_force));
   }

   b.AddDomainIntegrator(new VectorBoundaryLFIntegrator(force));
   b.Assemble();

   K->FormLinearSystem(ess_tdof_list, psi, b, A, X, B);

   so I will subtract B from z.

>>>using this fix in the full code gives me errors associated with memory, I think I did something very crazy.

Tzanio sent me the working code. What is the difference between AddBdrFaceIntegrator and AddBoundaryIntegrator?


3/13
the explicit code is working, however (they are probably all related to the boundary)
the current is still not right,
the boundary condition is not right

note the boundary condition for current should be 0, which is wrong now
If I turned off background B/Phi, then the current solver is right!
Reason:
M*J=K*Psi; however J=0 along the boundary; K*Psi=Psi along the boundary which is inconsistent, I need subtract those off

Luis: if we can figure out how to do it matrix-free in the explicit scheme, we should be good for the implicit scheme. The only matrix that needs to assemble is the mass and stiffness matrix (once in the how solve)

Q:
what solvers for mass and stiffness matrices I am using?

2019/3/6
the stucture of explicit scheme seems clear
the linear parts seem ok
the nonlinear term requires some work:
1. for explicit scheme, it is okay: I need to update matrix inside time step (with proper assembling, I am still not sure about how to do it). both PDSolver and exMHD need some work.
2. for nonlinear problem, how could link nonlinear term properly??

2019/3/3
move back to MFEM

>>>plot solutions at each time step (see ex16)
first open a glvis as a server in another terminal
then run the program; it should plot results auto

11/1
some useful info for developer:
https://github.com/mfem/mfem/blob/master/CONTRIBUTING.md#code-overview

10/30/2018
I believe I could use FE solver by providing the right hand side in two updates

Then I could built right side separatedly
